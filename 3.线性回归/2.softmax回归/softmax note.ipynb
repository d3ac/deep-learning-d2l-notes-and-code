{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}=softmax(o)$\n",
    "\n",
    "$softmax$ 函数为：$\\hat{y_i}=\\frac{e^{o_i}}{\\sum_k e^{o_k}}$ $o_i$ 代表的是第 $i$ 件事情发生的概率，这样一来，所有的 $\\hat{y_i}$ 相加等于 $1$\n",
    "\n",
    "我们使用概率 $y$ 和概率 $\\hat{y}$ 的区别作为损失，交叉熵常用来衡量两个概率之间的区别，交叉熵：$H(p,q)=\\sum_{i}-p_ilog(q_i)$\n",
    "\n",
    "将他作为损失之后，损失函数：$l(y,\\hat{y})=-\\sum_iy_ilog\\hat{y_i}$，又由于 $y$ 中只有 $1$ 个为 $1$ 其余全部为 $0$ 所以 $l(y,\\hat{y})=-log\\hat{y_y}$,所以损失只和预测出来的那个概率有关, 与其他的值无关\n",
    "\n",
    "梯度: $\\partial_{o_i}l(y,\\hat{y})=softmax(o)_i-y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$softmax$ 回归相比于 $sigmoid$ 回归来说, $softmax$ 一般做的是多分类的一个问题, $sigmoid$ 一般是二分类问题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81e75cef6ba1b10275bdce9a4ca9089c470de9a530eb43eaa03a6734d261d4ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
