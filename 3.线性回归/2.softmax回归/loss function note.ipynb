{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Funtion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一种 $Loss$ $Function$ 是 $L_2$ 范数: $l(y,\\hat{y})=\\frac{1}{2}(y-\\hat{y})^2$\n",
    "\n",
    "![1](picture/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蓝色线: 损失函数\n",
    "\n",
    "绿色线: 似然函数\n",
    "\n",
    "橙色线: 损失函数导数\n",
    "\n",
    "在这里, 离损失函数为 $0$ 的点越远, 导数的值就越大, 意味着随机梯度下降的时候就会走得更快, 但是有时这是一件坏事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二种 $Loss$ $Function$ 是 $L_1$ 范数: $l(y,\\hat{y})=\\left|y-\\hat{y}\\right|$\n",
    "\n",
    "![2](picture/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样的好处是无论何时, 导数都是不变的, 坏处是在 $0$ 处不可导, 并且在训练快达到收敛值的时侯可能会反复横跳导致出现异常"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三种 $Loss$ $Function$ 是 $Huber's$ $Robusts$ $Loss$:\n",
    "\n",
    "$$\n",
    "l(y,\\hat{y})=\\begin{cases}\n",
    "\\left|y-\\hat{y}\\right|-\\frac{1}{2} & if \\left|y-\\hat{y}\\right|>1 \\\\\n",
    "\\frac{1}{2}(y-\\hat{y})^2 & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "![3](picture/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合两个函数的优点"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
