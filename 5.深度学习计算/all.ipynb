{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层和块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import d2l.torch as d2l\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自己定义一个二层 MLP 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_input, num_hidden, num_output):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = nn.Linear(num_input, num_hidden)\n",
    "        self.ReLu_layer = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(num_hidden, num_output)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.output_layer(self.ReLu_layer(self.hidden_layer(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2245, -0.0217,  0.2890, -0.2601, -0.2096,  0.1335, -0.0225, -0.0997,\n",
      "         -0.0738, -0.0008]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1,30)\n",
    "net = MLP(30,40,10)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个 MLP 模型可以在 Sequential 里面混用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0767]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(MLP(30, 40, 10), nn.Linear(10, 1))\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1957]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = MySequential(MLP(30, 40, 10), nn.Linear(10, 1))\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先随便定义一个 $net$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两个东西, 第一个就是 $weight$ , 第二个是 $bias$ , 然后 `net[2]` 就是 $net$ 的第二个 $Linear$ 层 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1178, -0.3076, -0.1768, -0.2541, -0.2558, -0.2989,  0.1587,  0.0875]])), ('bias', tensor([0.3281]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Parameter$ 是 $torch$ 里面的一个可以优化的参数\n",
    "\n",
    "直接打印就会有数据和其他的值, 因为有梯度和其他啥的, 所以要通过 $data$ 来进行访问元素\n",
    "\n",
    "因为我们还没有计算, 所以这个地方没有梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.3281], requires_grad=True)\n",
      "tensor([0.3281])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "print(net[2].bias.grad == None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 $named\\_parameters()$ , 和 $state\\_dict()$ 函数来看参数, 注意这里没有 $ReLU$ 的参数, 因为这种层没有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n",
      "tensor([[ 0.3162, -0.0132, -0.4074, -0.3726],\n",
      "        [ 0.2250,  0.0811,  0.3649, -0.0370],\n",
      "        [ 0.0021, -0.1292,  0.1386, -0.2706],\n",
      "        [ 0.2814,  0.4665, -0.3999, -0.3446],\n",
      "        [ 0.4421, -0.3497,  0.2114, -0.4226],\n",
      "        [-0.2520,  0.2967, -0.3089, -0.2632],\n",
      "        [-0.3439, -0.4949,  0.1946, -0.1019],\n",
      "        [-0.3394,  0.1886, -0.4394,  0.4570]])\n",
      "tensor([-0.1434,  0.3631,  0.2792, -0.1560, -0.1792, -0.0491, -0.4767,  0.0508])\n"
     ]
    }
   ],
   "source": [
    "print(*((name, param.shape) for name, param in net.named_parameters()))\n",
    "print(net.state_dict()['0.weight'])\n",
    "print(net.state_dict()['0.bias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先随便定义一些网络层, 然后来看一下他们长什么样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_net1 = nn.Sequential(nn.Linear(10, 8), nn.ReLU(), nn.Linear(8, 6))\n",
    "small_net2 = nn.Sequential(nn.Linear(8, 6), nn.ReLU(), nn.Linear(6, 4))\n",
    "small_net3 = nn.Sequential(nn.Linear(4, 2), nn.ReLU(), nn.Linear(2, 1))\n",
    "all_net = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挨个输出看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=6, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=6, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=6, out_features=4, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(small_net1)\n",
    "print(small_net2)\n",
    "print(small_net3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以给这些 $module$ 取名, 默认是从 $0...n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (block_1): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=6, bias=True)\n",
      "  )\n",
      "  (block_2): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=6, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=6, out_features=4, bias=True)\n",
      "  )\n",
      "  (block_3): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "all_net.add_module('block_1', small_net1)\n",
    "all_net.add_module('block_2', small_net2)\n",
    "all_net.add_module('block_3', small_net3)\n",
    "print(all_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何来初始化默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1112, -0.2360,  0.2692,  0.1939,  0.2420,  0.2507, -0.0827,  0.1366,\n",
       "           0.2814, -0.3108],\n",
       "         [-0.2188,  0.0254,  0.2787,  0.2183,  0.1600,  0.1530, -0.3022,  0.2801,\n",
       "          -0.1571,  0.2410],\n",
       "         [ 0.2288, -0.0040,  0.0439, -0.2514, -0.0246,  0.1456,  0.1470,  0.0927,\n",
       "          -0.0633,  0.2097],\n",
       "         [ 0.0089, -0.0130,  0.0644,  0.2716,  0.0814,  0.2445, -0.0807, -0.3156,\n",
       "          -0.0203, -0.1829],\n",
       "         [ 0.1457, -0.1785, -0.3043, -0.1521, -0.0594, -0.2300, -0.1402, -0.0358,\n",
       "          -0.1389, -0.2248],\n",
       "         [ 0.2265,  0.2877,  0.0612, -0.0908,  0.1627, -0.1247, -0.0958,  0.2580,\n",
       "          -0.3087,  0.1263],\n",
       "         [ 0.1166,  0.1267, -0.2627, -0.1237, -0.0869,  0.2036,  0.0854,  0.2087,\n",
       "           0.2176,  0.2398],\n",
       "         [ 0.1284, -0.1467,  0.2669,  0.2055,  0.1070,  0.1434, -0.2259, -0.2988,\n",
       "          -0.1324,  0.1335]]),\n",
       " tensor([ 0.2783, -0.1530,  0.2433,  0.0174,  0.2770,  0.2692,  0.1059,  0.2805]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "all_net.apply(init_normal)\n",
    "all_net[0][0].weight.data, all_net[0][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对不同的层运用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0187,  0.3002, -0.0713, -0.5118, -0.4220,  0.0839, -0.3169,  0.0991,\n",
      "        -0.2650,  0.1380])\n",
      "tensor([42., 42., 42., 42., 42., 42., 42., 42.])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def const_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "all_net[0].apply(xavier)\n",
    "all_net[1].apply(const_init)\n",
    "print(all_net[0][0].weight.data[0])\n",
    "print(all_net[1][0].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数绑定, 共同使用一个层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(10, 8), nn.ReLU(),\n",
    "    shared, nn.ReLU(),\n",
    "    shared, nn.ReLU(),\n",
    "    nn.Linear(8,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6967,  1.5170, -1.5244],\n",
       "        [-0.0928,  0.4322, -1.1658]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, out_units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, out_units))\n",
    "        self.bias = nn.Parameter(torch.randn(out_units))\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.weight.data) + self.bias.data\n",
    "\n",
    "dense = MyLinear(5, 3)\n",
    "dense(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存一个 $list$ 或者 $dict$ 也可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print(x)\n",
    "torch.save(x, 'x-file')\n",
    "x = torch.load('x-file')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个网络来说, $torch$ 只能储存网络中的数据, 网络的结构需要自己存下来\n",
    "\n",
    "$state\\_dict$ 是一个 $OrderedDict$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        super().__init__()\n",
    "        self.params = nn.Parameter(torch.randn(input_units, output_units))\n",
    "        self.bias = nn.Parameter(torch.randn(output_units))\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.params) + self.bias.data\n",
    "\n",
    "def get_net():\n",
    "    return nn.Sequential(MyLinear(10,5), nn.ReLU(), MyLinear(5,1))\n",
    "net = get_net()\n",
    "torch.save(net.state_dict(), 'MySequential.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True])\n"
     ]
    }
   ],
   "source": [
    "net2 = get_net()\n",
    "net2.load_state_dict(torch.load('MySequential.params'))\n",
    "X = torch.rand(10)\n",
    "print(net2(X) == net(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81e75cef6ba1b10275bdce9a4ca9089c470de9a530eb43eaa03a6734d261d4ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
